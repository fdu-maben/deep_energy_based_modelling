{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "# load data\n",
    "# define model\n",
    "# define loss function\n",
    "# def training loop\n",
    "# def inference\n",
    "# def hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import lightning.pytorch as pl\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define activation function\n",
    "class Swish(nn.Module):\n",
    "    def forward(self,x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "class Sigmoid(nn.Module):\n",
    "    def forward(self,x):\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class ReLU(nn.Module):\n",
    "    def forward(self,x):\n",
    "        return torch.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model:\n",
    "# model list :\n",
    "# 1. joint energy based model\n",
    "# 2. energy based model\n",
    "class JointEnergyBasedModel(nn.Module):\n",
    "    def __init__(self,img_shape,hidden_features,num_classes):\n",
    "        super(JointEnergyBasedModel,self).__init__()\n",
    "        self.img_shape = img_shape\n",
    "        c_hid1 = hidden_features // 2\n",
    "        c_hid2 = hidden_features\n",
    "        c_hid3 = hidden_features * 2\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(1,c_hid1,kernel_size = 5, stride = 2, padding = 4),\n",
    "            Swish(),\n",
    "            nn.Conv2d(c_hid1,c_hid2,kernel_size = 3, stride = 2, padding = 1),\n",
    "            Swish(),\n",
    "            nn.Conv2d(c_hid2,c_hid3,kernel_size = 3, stride = 2, padding = 1),\n",
    "            Swish(),\n",
    "            nn.Conv2d(c_hid3,c_hid3,kernel_size = 3, stride = 2, padding = 1),\n",
    "            Swish(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc_layers = nn.Linear(c_hid3 * 4, num_classes)\n",
    "        self.device = \"cuda:0\"\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.cnn_layers(x)\n",
    "        logits = self.fc_layers(x)\n",
    "        energy = - torch.logsumexp(logits,dim = -1)\n",
    "        return energy, logits\n",
    "\n",
    "    def unconditional_sampling(self,num_steps,start_point,epsilon):\n",
    "        # sample with MCMC\n",
    "        if start_point is None:\n",
    "            start_point = torch.randn(self.img_shape, requires_grad = True, device=self.device)\n",
    "        else:\n",
    "             start_point.requires_grad = True\n",
    "        \n",
    "        start_point.data.clamp_(min=-1.0, max=1.0)\n",
    "        sample_list = [start_point]\n",
    "        with torch.no_grad():\n",
    "            energy_list = [self(start_point)[0].detach()]\n",
    "        for i in range(num_steps):\n",
    "            energy,logits = self(start_point)\n",
    "            energy.sum().backward()\n",
    "            start_point.grad.data.clamp_(-0.03, 0.03)\n",
    "            #start_point = start_point - pow(epsilon,2) / 2 * start_point.grad + epsilon * torch.randn_like(start_point)\n",
    "            start_point = start_point - 10 * start_point.grad.data + torch.randn_like(start_point).normal_(mean=0.0, std=0.005)\n",
    "            start_point = start_point.detach().requires_grad_(True)\n",
    "            start_point.data.clamp_(min=-1.0, max=1.0)\n",
    "            sample_list.append(start_point.clone())\n",
    "            energy_list.append(energy.detach())\n",
    "        \n",
    "        return start_point,sample_list,energy_list\n",
    "\n",
    "    def conditional_sampling(self,num_steps,start_point,target_class,epsilon):\n",
    "        # sample with MCMC\n",
    "        if start_point is None:\n",
    "            start_point = torch.randn(self.img_shape,requires_grad = True, device=self.device)\n",
    "        else:\n",
    "            start_point.requires_grad = True\n",
    "        start_point.data.clamp_(min=-1.0, max=1.0)\n",
    "        sample_list = [start_point]\n",
    "        with torch.no_grad():\n",
    "            energy_list = [self(start_point)[0].detach()]\n",
    "        for i in range(num_steps):\n",
    "            energy,logits = self(start_point)\n",
    "            logits[::,target_class].backward()\n",
    "            start_point.grad.data.clamp_(-0.03, 0.03)\n",
    "            start_point = start_point - pow(epsilon,2) / 2 * start_point.grad + epsilon * torch.randn_like(start_point)\n",
    "            start_point = start_point.detach().requires_grad_(True)\n",
    "            start_point.data.clamp_(min=-1.0, max=1.0)\n",
    "            sample_list.append(start_point)\n",
    "            energy_list.append(energy.detach())\n",
    "        \n",
    "        return start_point,sample_list,energy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pytorch lightning model (trainer)\n",
    "class PL_JointEnergyBasedModel(pl.lightning.LightningModule):\n",
    "    def __init__(self,model,method,num_steps,num_samples,start_point,target_class,epsilon):\n",
    "        super(PL_JointEnergyBasedModel,self).__init__(\n",
    "        )\n",
    "        self.model = model\n",
    "        self.method = method # training method including CD, PCD, SM, DSM, SSM, NEC etc.\n",
    "        self.num_steps = num_steps\n",
    "        self.num_samples = num_samples\n",
    "        self.start_point = start_point\n",
    "        self.target_class = target_class\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        pass\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        pass\n",
    "    \n",
    "    def test_step(self,batch,batch_idx):\n",
    "        pass\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        pass\n",
    "    \n",
    "    def on_train_epoch_start(self):\n",
    "        pass\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        pass\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        pass\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        pass\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definie CD training method\n",
    "\n",
    "def MLE_training(model,train_loader,val_loader,test_loader,num_epochs,batch_size,device,num_steps,num_samples,start_point,target_class,epsilon):\n",
    "    pass\n",
    "\n",
    "def CD_training(model,optimizer,train_loader,val_loader,test_loader,num_epochs,device,num_steps,epsilon,alpha):\n",
    "    cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for real_imgs, labels in train_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            real_imgs, labels = real_imgs.to(device), labels.to(device)\n",
    "            \n",
    "            # sample fake images with k steps of langevin dynamics\n",
    "            #fake_imgs , _ , _ = model.unconditional_sampling(num_steps, real_imgs + 0.005 * torch.randn_like(real_imgs),epsilon)\n",
    "\n",
    "            fake_imgs , _ , _ = model.unconditional_sampling(num_steps, 0.005 * torch.randn_like(real_imgs),epsilon)\n",
    "            # calculate the gradient of real images\n",
    "            energy_real, logits_real = model(real_imgs)\n",
    "            # calculate the gradient of fake images\n",
    "            energy_fake, logits_fake = model(fake_imgs)\n",
    "\n",
    "            reg_loss = alpha * (energy_real ** 2 + energy_fake ** 2).mean()\n",
    "\n",
    "            loss = energy_fake.mean() - energy_real.mean() + cross_entropy_loss(logits_real,labels) + reg_loss\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        total_energy_real = 0\n",
    "        total_energy_fake = 0\n",
    "        acc_list = []\n",
    "\n",
    "        for real_imgs, labels in val_loader:\n",
    "\n",
    "                \n",
    "            real_imgs, labels = real_imgs.to(device), labels.to(device)\n",
    "\n",
    "            fake_imgs , _ , _ = model.unconditional_sampling(num_steps, real_imgs + 0.01 * torch.randn_like(real_imgs),epsilon)\n",
    "\n",
    "            energy_real, logits_real = model(real_imgs)\n",
    "\n",
    "            energy_fake, logits_fake = model(fake_imgs)\n",
    "            \n",
    "            reg_loss = alpha * (energy_real ** 2 + energy_fake ** 2).mean()\n",
    "\n",
    "            loss = energy_fake.mean() - energy_real.mean() + cross_entropy_loss(logits_real,labels) + reg_loss\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            total_energy_real += energy_real.sum().item()\n",
    "            total_energy_fake += energy_fake.sum().item()\n",
    "\n",
    "            acc_list.append(torch.argmax(logits_real,dim = -1) == labels)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} : val loss: {val_loss/len(val_loader)}\")\n",
    "        print(f\"Epoch {epoch+1} : energy_real: {total_energy_real/len(val_loader)}\")\n",
    "        print(f\"Epoch {epoch+1} : energy_fake: {total_energy_fake/len(val_loader)}\")\n",
    "        print(f\"Epoch {epoch+1} : acc: {torch.cat(acc_list).float().mean()}\")\n",
    "        \n",
    "def PCD_training(model,num_steps,num_samples,start_point,target_class,epsilon):\n",
    "\n",
    "    pass\n",
    "\n",
    "def CD_replay_buffer_training(model,optimizer,train_loader,val_loader,test_loader,num_epochs,device,num_steps,epsilon,alpha):\n",
    "    cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for real_imgs, labels in train_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            real_imgs, labels = real_imgs.to(device), labels.to(device)\n",
    "            \n",
    "            # sample fake images with k steps of langevin dynamics\n",
    "            fake_imgs , _ , _ = model.unconditional_sampling(num_steps, real_imgs + 0.01 * torch.randn_like(real_imgs),epsilon)\n",
    "\n",
    "            # calculate the gradient of real images\n",
    "            energy_real, logits_real = model(real_imgs)\n",
    "            # calculate the gradient of fake images\n",
    "            energy_fake, logits_fake = model(fake_imgs)\n",
    "\n",
    "            reg_loss = alpha * (energy_real ** 2 + energy_fake ** 2).mean()\n",
    "\n",
    "            loss = energy_fake.mean() - energy_real.mean() + cross_entropy_loss(logits_real,labels) + reg_loss\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        total_energy_real = 0\n",
    "        total_energy_fake = 0\n",
    "        acc_list = []\n",
    "\n",
    "        for real_imgs, labels in val_loader:\n",
    "\n",
    "                \n",
    "            real_imgs, labels = real_imgs.to(device), labels.to(device)\n",
    "\n",
    "            fake_imgs , _ , _ = model.unconditional_sampling(num_steps, real_imgs + 0.01 * torch.randn_like(real_imgs),epsilon)\n",
    "\n",
    "            energy_real, logits_real = model(real_imgs)\n",
    "\n",
    "            energy_fake, logits_fake = model(fake_imgs)\n",
    "            \n",
    "            reg_loss = alpha * (energy_real ** 2 + energy_fake ** 2).mean()\n",
    "\n",
    "            loss = energy_fake.mean() - energy_real.mean() + cross_entropy_loss(logits_real,labels) + reg_loss\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            total_energy_real += energy_real.sum().item()\n",
    "            total_energy_fake += energy_fake.sum().item()\n",
    "\n",
    "            acc_list.append(torch.argmax(logits_real,dim = -1) == labels)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} : val loss: {val_loss/len(val_loader)}\")\n",
    "        print(f\"Epoch {epoch+1} : energy_real: {total_energy_real/len(val_loader)}\")\n",
    "        print(f\"Epoch {epoch+1} : energy_fake: {total_energy_fake/len(val_loader)}\")\n",
    "        print(f\"Epoch {epoch+1} : acc: {torch.cat(acc_list).float().mean()}\")\n",
    "\n",
    "def SM_training(model,num_steps,num_samples,start_point,target_class,epsilon):\n",
    "    \n",
    "    pass\n",
    "\n",
    "def DSM_training(model,num_steps,num_samples,start_point,target_class,epsilon):\n",
    "    pass\n",
    "\n",
    "def SSM_training(model,num_steps,num_samples,start_point,target_class,epsilon):\n",
    "    pass\n",
    "\n",
    "def NEC_training(model,num_steps,num_samples,start_point,target_class,epsilon):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "img_shape = (1, 28, 28)\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "hidden_features = 32\n",
    "num_steps = 10\n",
    "num_samples = 1\n",
    "start_point = None\n",
    "target_class = None\n",
    "epsilon = 0.01\n",
    "lr = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5,0.5)\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"/work/home/maben/project/blue_whale_lab/projects/pareto_ebm/datasets\", train=True, transform=transform, download=True)\n",
    "val_dataset = datasets.MNIST(root=\"/work/home/maben/project/blue_whale_lab/projects/pareto_ebm/datasets\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train_dataset: 60000\n",
      "length of val_dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"length of train_dataset:\",len(train_dataset))\n",
    "print(\"length of val_dataset:\",len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display some images\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15, 3))  # 调整图像大小，宽度更大以适应10列\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(train_dataset[i][0].squeeze(), cmap='gray')\n",
    "    plt.axis('off')  # 隐藏坐标轴\n",
    "    plt.title(f\"label: {train_dataset[i][1]}\")\n",
    "plt.tight_layout()  # 自动调整子图间距\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = JointEnergyBasedModel(img_shape,hidden_features,num_classes).to(\"cuda:0\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sampler] Epoch 1 : val loss: 154058195.64556962\n",
      "[Sampler] Epoch 1 : energy_real: -8215.941524650478\n",
      "[Sampler] Epoch 1 : energy_fake: -4916243.217563291\n",
      "[Sampler] Epoch 1 : acc: 0.11629999428987503\n",
      "[Sampler] Epoch 2 : val loss: 143576852.35443038\n",
      "[Sampler] Epoch 2 : energy_real: 333.9695171887362\n",
      "[Sampler] Epoch 2 : energy_fake: -4679781.224683545\n",
      "[Sampler] Epoch 2 : acc: 0.10740000009536743\n",
      "[Sampler] Epoch 3 : val loss: 142076404.15189874\n",
      "[Sampler] Epoch 3 : energy_real: 1518.7467153042178\n",
      "[Sampler] Epoch 3 : energy_fake: -4641759.223892405\n",
      "[Sampler] Epoch 3 : acc: 0.10359999537467957\n",
      "[Sampler] Epoch 4 : val loss: 157678730.32911393\n",
      "[Sampler] Epoch 4 : energy_real: 3410.0186783030063\n",
      "[Sampler] Epoch 4 : energy_fake: -4892284.107594937\n",
      "[Sampler] Epoch 4 : acc: 0.10199999809265137\n",
      "[Sampler] Epoch 5 : val loss: 155422081.01265824\n",
      "[Sampler] Epoch 5 : energy_real: 940.200459250921\n",
      "[Sampler] Epoch 5 : energy_fake: -4820898.286392405\n",
      "[Sampler] Epoch 5 : acc: 0.10209999978542328\n"
     ]
    }
   ],
   "source": [
    "CD_training_with_replay_buffer_sampler(model,optimizer,train_loader,val_loader,val_loader,5,\"cuda:0\",60,0.01,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : val loss: 193567904.81012657\n",
      "Epoch 1 : energy_real: -16686.518931714796\n",
      "Epoch 1 : energy_fake: -5562215.143987342\n",
      "Epoch 1 : acc: 0.17099998891353607\n",
      "Epoch 2 : val loss: 288480768.8101266\n",
      "Epoch 2 : energy_real: -578570.8679291931\n",
      "Epoch 2 : energy_fake: -6767223.125\n",
      "Epoch 2 : acc: 0.13379999995231628\n",
      "Epoch 3 : val loss: 133316242.73417722\n",
      "Epoch 3 : energy_real: -78053.66913382614\n",
      "Epoch 3 : energy_fake: -4550643.13449367\n",
      "Epoch 3 : acc: 0.13419999182224274\n",
      "Epoch 4 : val loss: 107100647.59493671\n",
      "Epoch 4 : energy_real: -57630.18251520471\n",
      "Epoch 4 : energy_fake: -4068489.148734177\n",
      "Epoch 4 : acc: 0.14259999990463257\n",
      "Epoch 5 : val loss: 127640337.51898734\n",
      "Epoch 5 : energy_real: -98803.91957884197\n",
      "Epoch 5 : energy_fake: -4420160.409018988\n",
      "Epoch 5 : acc: 0.1184999942779541\n"
     ]
    }
   ],
   "source": [
    "CD_training(model,optimizer,train_loader,val_loader,val_loader,5,\"cuda:0\",60,0.01,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CD_training_with_replay_buffer(\n",
    "    model, optimizer, train_loader, val_loader, test_loader, num_epochs, device, \n",
    "    num_steps, epsilon, alpha, buffer_size=10000\n",
    "):\n",
    "    cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # 1. 初始化 replay buffer\n",
    "    # 设定 buffer 为 real_imgs 的 shape\n",
    "    buffer_shape = (buffer_size, ) + next(iter(train_loader))[0].shape[1:]\n",
    "    replay_buffer = torch.randn(buffer_shape, device=device)\n",
    "\n",
    "    buffer_ptr = 0\n",
    "\n",
    "    def fetch_from_buffer(batch_size):\n",
    "        idx = torch.randint(0, buffer_size, (batch_size,), device=device)\n",
    "        return replay_buffer[idx], idx\n",
    "\n",
    "    def update_buffer(samples, idx):\n",
    "        replay_buffer[idx] = samples.detach()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for real_imgs, labels in train_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            real_imgs, labels = real_imgs.to(device), labels.to(device)\n",
    "            batch_size = real_imgs.size(0)\n",
    "\n",
    "            # ---- Replay Buffer采样 ----\n",
    "            # 2. 从buffer取 batch_size 个初始负样本\n",
    "            fake_init, buffer_idx = fetch_from_buffer(batch_size)\n",
    "            # 3. Langevin 采样生成fake_imgs\n",
    "            fake_imgs, _, _ = model.unconditional_sampling(num_steps, fake_init, epsilon)\n",
    "            # 4. 更新buffer\n",
    "            update_buffer(fake_imgs, buffer_idx)\n",
    "\n",
    "            # --- 损失计算 ---\n",
    "            energy_real, logits_real = model(real_imgs)\n",
    "            energy_fake, logits_fake = model(fake_imgs)\n",
    "\n",
    "            reg_loss = alpha * (energy_real ** 2 + energy_fake ** 2).mean()\n",
    "\n",
    "            loss = energy_fake.mean() - energy_real.mean() + cross_entropy_loss(logits_real, labels) + reg_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # ---- Validation ----\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        total_energy_real = 0\n",
    "        total_energy_fake = 0\n",
    "        acc_list = []\n",
    "\n",
    "        for real_imgs, labels in val_loader:\n",
    "            real_imgs, labels = real_imgs.to(device), labels.to(device)\n",
    "            batch_size = real_imgs.size(0)\n",
    "\n",
    "            # 验证时同样用 buffer 采样\n",
    "            fake_init, buffer_idx = fetch_from_buffer(batch_size)\n",
    "            fake_imgs, _, _ = model.unconditional_sampling(num_steps, fake_init, epsilon)\n",
    "            update_buffer(fake_imgs, buffer_idx)\n",
    "\n",
    "            energy_real, logits_real = model(real_imgs)\n",
    "            energy_fake, logits_fake = model(fake_imgs)\n",
    "\n",
    "            reg_loss = alpha * (energy_real ** 2 + energy_fake ** 2).mean()\n",
    "\n",
    "            loss = energy_fake.mean() - energy_real.mean() + cross_entropy_loss(logits_real, labels) + reg_loss\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            total_energy_real += energy_real.sum().item()\n",
    "            total_energy_fake += energy_fake.sum().item()\n",
    "            acc_list.append(torch.argmax(logits_real, dim=-1) == labels)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} : val loss: {val_loss/len(val_loader)}\")\n",
    "        print(f\"Epoch {epoch+1} : energy_real: {total_energy_real/len(val_loader)}\")\n",
    "        print(f\"Epoch {epoch+1} : energy_fake: {total_energy_fake/len(val_loader)}\")\n",
    "        print(f\"Epoch {epoch+1} : acc: {torch.cat(acc_list).float().mean()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
